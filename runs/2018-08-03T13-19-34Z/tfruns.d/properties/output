
> #' Trains a simple deep NN on the MNIST dataset.
> #'
> #' Gets to 98.40% test accuracy after 20 epochs (there is *a lot* of margin for
> #' paramet .... [TRUNCATED] 

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("dropout1", 0.4),
+   flag_numeric( .... [TRUNCATED] 

> # Data Preparation ---------------------------------------------------
> 
> # The data, shuffled and split between train and test sets
> mnist <- da .... [TRUNCATED] 

> x_train <- mnist$train$x

> y_train <- mnist$train$y

> x_test <- mnist$test$x

> y_test <- mnist$test$y

> # Reshape
> dim(x_train) <- c(nrow(x_train), 784)

> dim(x_test) <- c(nrow(x_test), 784)

> # Transform RGB values into [0,1] range
> x_train <- x_train / 255

> x_test <- x_test / 255

> # Convert class vectors to binary class matrices
> y_train <- to_categorical(y_train, 10)

> y_test <- to_categorical(y_test, 10)

> # Define Model --------------------------------------------------------------
> 
> model <- keras_model_sequential()

> model %>%
+   layer_dense(units = 60, activation = 'relu', input_shape = c(784)) %>%
+   layer_dropout(rate = FLAGS$dropout1) %>%
+   layer_dense(un .... [TRUNCATED] 

> model %>% compile(
+   loss = 'categorical_crossentropy',
+   optimizer = optimizer_rmsprop(lr = 0.001),
+   metrics = c('accuracy')
+ )

> # Training & Evaluation ----------------------------------------------------
> 
> history <- model %>% fit(
+   x_train, y_train,
+   batch_size = 1 .... [TRUNCATED] 

> plot(history)

> score <- model %>% evaluate(
+   x_test, y_test,
+   verbose = 0
+ )

> cat('Test loss:', score$loss, '\n')
Test loss: 0.1248896 

> cat('Test accuracy:', score$acc, '\n')
Test accuracy: 0.9667 
