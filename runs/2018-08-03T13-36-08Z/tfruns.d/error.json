{
  "message": "object of type 'closure' is not subsettable",
  "traceback": ["create_layer(keras$layers$Dense, object, list(units = as.integer(units), \n    activation = activation, use_bias = use_bias, kernel_initializer = kernel_initializer, \n    bias_initializer = bias_initializer, kernel_regularizer = kernel_regularizer, \n    bias_regularizer = bias_regularizer, activity_regularizer = activity_regularizer, \n    kernel_constraint = kernel_constraint, bias_constraint = bias_constraint, \n    input_shape = normalize_shape(input_shape), batch_input_shape = normalize_shape(batch_input_shape), \n    batch_size = as_nullable_integer(batch_size), dtype = dtype, \n    name = name, trainable = trainable, weights = weights))", "layer_dense(., units = FLAGS$l1units, activation = \"relu\", input_shape = c(ncol(exp)))", "function_list[[i]](value)", "freduce(value, `_function_list`)", "`_fseq`(`_lhs`)", "eval(quote(`_fseq`(`_lhs`)), env, env)", "eval(quote(`_fseq`(`_lhs`)), env, env)", "withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))", "model %>% layer_dense(units = FLAGS$l1units, activation = \"relu\", \n    input_shape = c(ncol(exp))) %>% layer_dense(units = FLAGS$l2units, \n    activation = \"relu\") %>% layer_dense(units = 5, activation = \"softmax\")", "eval(ei, envir)", "eval(ei, envir)", "withVisible(eval(ei, envir))", "tuning_run(\"epi_mlp.R\", flags = list(l1units = c(50, 100, 150), \n    l2units = c(50, 100, 150)))"]
}
