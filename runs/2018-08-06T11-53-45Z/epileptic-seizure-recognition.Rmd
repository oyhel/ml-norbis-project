---
title: "Epileptic Seizure Recognition"
author: "Ã˜yvind Helgeland"
date: "August 2, 2018"
output: html_document
---

```{r}
# Data: http://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition

# Tutorials
# https://github.com/apurvnnd/Epileptic-Seizure-Recognition-Using-ANN/blob/master/ESR.py
# https://www.datacamp.com/community/tutorials/keras-r-deep-learning

```


```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(keras)
library(tfruns)
```


```{r}
# Load data
data <- read.table('epil-seizure.csv', header = T, stringsAsFactors = F, sep=',')

# Split data into to explanatory (exp) and response (res)
exp <- data.matrix(data[,2:179])
res <- data[,180]
```

```{r}
# To get some overview of the data separation, PCA is performed.
pc <- prcomp(data[,2:179])

t <- as.data.frame(pc$x)
t2 <- cbind(t,res)

ggplot(t) + 
  geom_point(aes(PC1,PC2, col=as.factor(res)), alpha = 0.5) + 
  scale_color_manual(values = palette()[3:7]) + 
  ggtitle("PC1 vs PC2, all groups")

ggplot(subset(t2, res!=1)) + 
  geom_point(aes(PC1,PC2, col=as.factor(res)), alpha = 0.5) + 
  scale_color_manual(values = palette()[4:7]) +
  ggtitle("PC1 vs PC2, group 2-5")


```

```{r}
# Split dataset into test and training
epi_x_train <- exp[1:10000,]
epi_y_train.cat <- res[1:10000]
epi_y_train <- to_categorical(epi_y_train.cat)[,2:6]


epi_x_test <- exp[10001:nrow(exp),]
epi_y_test.cat <- res[10001:nrow(exp)]
epi_y_test <- to_categorical(epi_y_test.cat)[,2:6]

```

```{r}
# Testing archictecture with two layers
runs <- tuning_run("epi_mlp_2layer.R", flags = list(
  l1units = c(50, 100, 150),
  l2units = c(50, 100, 150),
  dropout1 = c(0,0.2,0.4)
))

rundf <- runs[,c('eval_loss','eval_acc', 'metric_loss', 'metric_val_loss', 'metric_val_acc', 'flag_l1units', 'flag_l2units')]

knitr::kable(head(arrange(rundf, desc(eval_acc))))

```


```{r, cache=T}
# Testing architecture with 3 layers
runs <- tuning_run("epi_mlp.R", flags = list(
  l1units = c(50, 100, 150),
  l2units = c(50, 100, 150),
  l3units = c(50, 100, 150),
  dropout1 = c(0,0.2,0.4)
))
```

```{r}
rundf <- runs[,c('eval_loss','eval_acc', 'metric_loss', 'metric_val_loss', 'metric_val_acc', 'flag_l1units', 'flag_l2units', 'flag_l3units', 'flag_dropout1')]

knitr::kable(head(arrange(rundf, desc(eval_acc))))
```



```{r}
# generate model
model = keras_model_sequential() 
model %>%
  layer_dense(units = 100, activation = 'relu', input_shape = c(ncol(exp))) %>% 
  layer_dropout(rate = 0.2) %>% 
  layer_dense(units = 50, activation = 'relu') %>%
  layer_dense(units = 150, activation = 'relu') %>%
  layer_dense(units = 5, activation = 'softmax')

# View a smmary of the model
summary(model)

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

# fit the model
history <- model %>% fit(
  epi_x_train, epi_y_train, 
  epochs = 1000, 
  validation_split = 0.2,
  #callbacks = callback_tensorboard("logs/run_c"),
  verbose = 1
)

```

```{r}
classes <- model %>% predict_classes(epi_x_test)

# generate confusion matrix
cm <- table(epi_y_test.cat, classes+1)
print(cm)

cm.df <- as.data.frame(cm)
names(cm.df) <- c('actual','predict','freq')

plot <- ggplot(cm.df)
plot + geom_tile(aes(x=actual, y=predict, fill=freq)) +
  scale_x_discrete(name="Actual Class") +
  scale_y_discrete(name="Predicted Class") 
  #scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.2))
#print(plot)

#+  +  +  + labs(fill="Normalized\nFrequency")

```

```{r}
score <- model %>% evaluate(epi_x_test, epi_y_test)
print(score)
```

```{r}
library(tfruns)
training_run("mnist_mlp.R")

runs <- tuning_run("epi_mlp.R", flags = list(
  l1units = c(50, 100, 150),
  l2units = c(50, 100, 150),
  l3units = c(50, 100, 150),
  dropout1 = c(0,0.2,0.4)
))
```

